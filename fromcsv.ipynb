{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNn3es9R68R1lp7I3xUy3kg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alkang53/CNN-BASED-TEXT-CLASSIFICATION/blob/main/fromcsv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9J-KDLWZtwz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import time\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling2D, Dropout, Input, Embedding\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TTC4900 - kelimelerin orj halini içeren csv\n",
        "csv_file = '/content/drive/My Drive/7allV03.csv'\n",
        "data = pd.read_csv(csv_file)\n",
        "\n",
        "corpus = data['text']\n",
        "categoryNum = 7"
      ],
      "metadata": {
        "id": "gpqrq6w4aJrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TTC4900 - kelimelerin köklerini içeren csv\n",
        "csv_file = '/content/drive/My Drive/7allV03_lem.csv'\n",
        "data = pd.read_csv(csv_file)\n",
        "\n",
        "corpus = data['text']\n",
        "categoryNum = 7"
      ],
      "metadata": {
        "id": "40eVgl2rumz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#müşteri yorumları - kelimelerin orj halini içeren csv\n",
        "csv_file = '/content/drive/My Drive/e-ticaret_urun_yorumlari_as.csv'\n",
        "data = pd.read_csv(csv_file, sep=';')\n",
        "\n",
        "corpus = data['text']\n",
        "categoryNum = 2"
      ],
      "metadata": {
        "id": "eSSRaUy0r0Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#müşteri yorumları - kelimelerin köklerini içeren csv\n",
        "csv_file = '/content/drive/My Drive/e-ticaret_urun_yorumlari_lem.csv'\n",
        "data = pd.read_csv(csv_file, sep=';')\n",
        "\n",
        "corpus = data['text']\n",
        "categoryNum = 2"
      ],
      "metadata": {
        "id": "1PRe6j6wr0o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#+ Stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "for indexstop, row in enumerate(corpus):\n",
        "  tokens = word_tokenize(str(row))\n",
        "  filtered_text = [t for t in tokens if not t.lower() in stopwords.words(\"turkish\")]\n",
        "  corpus[indexstop] = \" \".join(filtered_text)\n",
        "  #corpus.replace(str(row), \" \".join(filtered_text))\n",
        "\n"
      ],
      "metadata": {
        "id": "wtGRPRb7jZht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "possible_labels = data.category.unique()\n",
        "\n",
        "label_dict = {}\n",
        "for index, possible_label in enumerate(possible_labels):\n",
        "    label_dict[possible_label] = index\n",
        "label_dict\n",
        "data['label'] = data.category.replace(label_dict)\n",
        "\n"
      ],
      "metadata": {
        "id": "2m0rKAFPTdvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TFIDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "xBig = vectorizer.fit_transform(corpus)\n",
        "y = data['label'].values\n",
        "\n"
      ],
      "metadata": {
        "id": "qEjq_brDqbTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#selectKbest\n",
        "#Kütüphaneleri yükleme\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "#Ki kare testine göre en iyi sonuç veren 2 özelliği seç\n",
        "features = SelectKBest(chi2, k = 2000)\n",
        "#features = SelectKBest(k = 2000)\n",
        "x = features.fit_transform(xBig, y)\n",
        "#Azaltılmış Özellikler\n",
        "print('Orjinal Özellik Sayısı:', xBig.shape[1])\n",
        "print('Seçilmiş Özellik Sayısı:', x.shape[1])"
      ],
      "metadata": {
        "id": "HSe4qYA1ehmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################naive bayes#############################\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "prec_per_fold = []\n",
        "recall_per_fold = []\n",
        "f1_per_fold = []\n",
        "confmat_per_fold = np.zeros((categoryNum, categoryNum),dtype=int)\n",
        "predictTimeTotal = 0;\n",
        "for train, test in kfold.split(x, y):\n",
        "  #veri seti: 5692 öznitelikten oluşan bir arff dosyası\n",
        "  modelNB = MultinomialNB()\n",
        "\n",
        "  # Generate a print\n",
        "  print(\"--\")\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  #train the model\n",
        "  history = modelNB.fit(x[train],y[train])\n",
        "\n",
        "  start = time.process_time();\n",
        "  y_pred = modelNB.predict(x[test]);\n",
        "  end = time.process_time();\n",
        "  ac=accuracy_score(y[test],y_pred);\n",
        "  acc_per_fold.append(ac);\n",
        "\n",
        "  cm = confusion_matrix(y[test], y_pred);\n",
        "\n",
        "  \n",
        "  # Print f1, precision, and recall scores\n",
        "  precision_score_ = precision_score(y[test], y_pred , average=\"macro\")\n",
        "  recall_score_ = recall_score(y[test], y_pred , average=\"macro\")\n",
        "  f1_score_ = f1_score(y[test], y_pred , average=\"macro\")\n",
        "  confmat_ = confusion_matrix(y[test],y_pred, labels=[0, 1, 2, 3, 4, 5, 6])\n",
        "  print(precision_score_)\n",
        "  print(recall_score_)\n",
        "  print(f1_score_)\n",
        "  print(confmat_)\n",
        "  prec_per_fold.append(precision_score_)\n",
        "  recall_per_fold.append(recall_score_)\n",
        "  f1_per_fold.append(f1_score_)\n",
        "  confmat_per_fold = np.add(confmat_per_fold, confmat_) \n",
        "  predictTimeTotal += end - start;\n",
        " \n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "print(\"acc: \", np.mean(acc_per_fold))\n",
        "print(\"prec: \", np.mean(prec_per_fold))\n",
        "print(\"recall: \", np.mean(recall_per_fold))\n",
        "print(\"f1: \", np.mean(f1_per_fold))\n",
        "print(\"confusion matrix:\")\n",
        "print(confmat_per_fold)\n",
        "print(\"predictTimeTotal: \", predictTimeTotal)"
      ],
      "metadata": {
        "id": "HA7yvI78hPt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJsDYgQqXGCZ"
      },
      "outputs": [],
      "source": [
        "#################random forest#############################\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "prec_per_fold = []\n",
        "recall_per_fold = []\n",
        "f1_per_fold = []\n",
        "confmat_per_fold = np.zeros((categoryNum, categoryNum),dtype=int)\n",
        "predictTimeTotal = 0;\n",
        "for train, test in kfold.split(x, y):\n",
        "  #veri seti: 5692 öznitelikten oluşan bir arff dosyası\n",
        "  modelRF = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "  # Generate a print\n",
        "  print(\"--\")\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  #train the model\n",
        "  history = modelRF.fit(x[train],y[train])\n",
        "\n",
        "  start = time.process_time();\n",
        "  y_pred = modelRF.predict(x[test]);\n",
        "  end = time.process_time();\n",
        "  ac=accuracy_score(y[test],y_pred);\n",
        "  acc_per_fold.append(ac);\n",
        "\n",
        "  cm = confusion_matrix(y[test], y_pred);\n",
        "\n",
        "  \n",
        "  # Print f1, precision, and recall scores\n",
        "  precision_score_ = precision_score(y[test], y_pred , average=\"macro\")\n",
        "  recall_score_ = recall_score(y[test], y_pred , average=\"macro\")\n",
        "  f1_score_ = f1_score(y[test], y_pred , average=\"macro\")\n",
        "  confmat_ = confusion_matrix(y[test],y_pred, labels=[0, 1, 2, 3, 4, 5, 6])\n",
        "  print(precision_score_)\n",
        "  print(recall_score_)\n",
        "  print(f1_score_)\n",
        "  print(confmat_)\n",
        "  prec_per_fold.append(precision_score_)\n",
        "  recall_per_fold.append(recall_score_)\n",
        "  f1_per_fold.append(f1_score_)\n",
        "  confmat_per_fold = np.add(confmat_per_fold, confmat_) \n",
        "  predictTimeTotal += end - start;\n",
        " \n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "print(\"acc: \", np.mean(acc_per_fold))\n",
        "print(\"prec: \", np.mean(prec_per_fold))\n",
        "print(\"recall: \", np.mean(recall_per_fold))\n",
        "print(\"f1: \", np.mean(f1_per_fold))\n",
        "print(\"confusion matrix:\")\n",
        "print(confmat_per_fold)\n",
        "print(\"predictTimeTotal: \", predictTimeTotal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYMvWOFv0H8w"
      },
      "outputs": [],
      "source": [
        "#################  DVM  #############################\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "prec_per_fold = []\n",
        "recall_per_fold = []\n",
        "f1_per_fold = []\n",
        "confmat_per_fold = np.zeros((categoryNum, categoryNum),dtype=int)\n",
        "predictTimeTotal = 0;\n",
        "for train, test in kfold.split(x, y):\n",
        "  #veri seti: 5692 öznitelikten oluşan bir arff dosyası\n",
        "  model = svm.SVC(kernel='linear') # Linear Kernel\n",
        "\n",
        "  # Generate a print\n",
        "  print(\"--\")\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  #train the model\n",
        "  history = model.fit(x[train],y[train])\n",
        "\n",
        "  start = time.process_time();\n",
        "  y_pred = model.predict(x[test]);\n",
        "  end = time.process_time();\n",
        "  ac=accuracy_score(y[test],y_pred);\n",
        "  acc_per_fold.append(ac);\n",
        "\n",
        "  cm = confusion_matrix(y[test], y_pred);\n",
        "\n",
        "  \n",
        "  # Print f1, precision, and recall scores\n",
        "  precision_score_ = precision_score(y[test], y_pred , average=\"macro\")\n",
        "  recall_score_ = recall_score(y[test], y_pred , average=\"macro\")\n",
        "  f1_score_ = f1_score(y[test], y_pred , average=\"macro\")\n",
        "  confmat_ = confusion_matrix(y[test],y_pred, labels=[0, 1, 2, 3, 4, 5, 6])\n",
        "  print(precision_score_)\n",
        "  print(recall_score_)\n",
        "  print(f1_score_)\n",
        "  print(confmat_)\n",
        "  prec_per_fold.append(precision_score_)\n",
        "  recall_per_fold.append(recall_score_)\n",
        "  f1_per_fold.append(f1_score_)\n",
        "  confmat_per_fold = np.add(confmat_per_fold, confmat_) \n",
        "  predictTimeTotal += end - start;\n",
        " \n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "print(\"acc: \", np.mean(acc_per_fold))\n",
        "print(\"prec: \", np.mean(prec_per_fold))\n",
        "print(\"recall: \", np.mean(recall_per_fold))\n",
        "print(\"f1: \", np.mean(f1_per_fold))\n",
        "print(\"confusion matrix:\")\n",
        "print(confmat_per_fold)\n",
        "print(\"predictTimeTotal: \", predictTimeTotal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CnWVO0w1YMK"
      },
      "outputs": [],
      "source": [
        "#################  knn  #############################\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "prec_per_fold = []\n",
        "recall_per_fold = []\n",
        "f1_per_fold = []\n",
        "confmat_per_fold = np.zeros((categoryNum, categoryNum),dtype=int)\n",
        "predictTimeTotal = 0;\n",
        "for train, test in kfold.split(x, y):\n",
        "  #veri seti: 5692 öznitelikten oluşan bir arff dosyası\n",
        "  modelKN = KNeighborsClassifier(n_neighbors = 5) # Linear Kernel\n",
        "\n",
        "  # Generate a print\n",
        "  print(\"--\")\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  #train the model\n",
        "  history = modelKN.fit(x[train],y[train])\n",
        "\n",
        "  start = time.process_time();\n",
        "  y_pred = modelKN.predict(x[test]);\n",
        "  end = time.process_time();\n",
        "  ac=accuracy_score(y[test],y_pred);\n",
        "  acc_per_fold.append(ac);\n",
        "\n",
        "  cm = confusion_matrix(y[test], y_pred);\n",
        "\n",
        "  \n",
        "  # Print f1, precision, and recall scores\n",
        "  precision_score_ = precision_score(y[test], y_pred , average=\"macro\")\n",
        "  recall_score_ = recall_score(y[test], y_pred , average=\"macro\")\n",
        "  f1_score_ = f1_score(y[test], y_pred , average=\"macro\")\n",
        "  confmat_ = confusion_matrix(y[test],y_pred, labels=[0, 1, 2, 3, 4, 5, 6])\n",
        "  print(precision_score_)\n",
        "  print(recall_score_)\n",
        "  print(f1_score_)\n",
        "  print(confmat_)\n",
        "  prec_per_fold.append(precision_score_)\n",
        "  recall_per_fold.append(recall_score_)\n",
        "  f1_per_fold.append(f1_score_)\n",
        "  confmat_per_fold = np.add(confmat_per_fold, confmat_) \n",
        "  predictTimeTotal += end - start;\n",
        " \n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "print(\"acc: \", np.mean(acc_per_fold))\n",
        "print(\"prec: \", np.mean(prec_per_fold))\n",
        "print(\"recall: \", np.mean(recall_per_fold))\n",
        "print(\"f1: \", np.mean(f1_per_fold))\n",
        "print(\"confusion matrix:\")\n",
        "print(confmat_per_fold)\n",
        "print(\"predictTimeTotal: \", predictTimeTotal)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = x.toarray()\n",
        "#x2 = x2.reshape(x2.shape[0], x2.shape[1], 1)\n",
        "max_length=x2.shape[1]\n",
        "vocab_size = x2.shape[1]"
      ],
      "metadata": {
        "id": "Ka3_mbEXlLAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "714Nv5qC3-qM"
      },
      "outputs": [],
      "source": [
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "prec_per_fold = []\n",
        "recall_per_fold = []\n",
        "f1_per_fold = []\n",
        "confmat_per_fold = np.zeros((categoryNum, categoryNum),dtype=int)\n",
        "predictTimeTotal = 0;\n",
        "for train, test in kfold.split(x2, y):\n",
        "  #veri seti: vocab_size öznitelikten oluşan bir arff dosyası\n",
        "  modelCNN = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Conv1D(128, 1, activation=\"relu\", input_shape=(vocab_size,1)), #dropout 0.2 91,5 droopout 0.05 88 dropout 0.3 89\n",
        "      tf.keras.layers.Flatten(input_shape=(vocab_size,1)),\n",
        "      tf.keras.layers.Dense(128,activation='relu'),\n",
        "      tf.keras.layers.Dropout(0.3),\n",
        "      tf.keras.layers.Dense(7)\n",
        "      ])\n",
        "\n",
        "\n",
        "  modelCNN.summary()\n",
        "  #define loss function variable\n",
        "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  \n",
        "  # compile the model\n",
        "  modelCNN.compile(optimizer='adam',\n",
        "             loss=loss_fn,\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  # Generate a print\n",
        "  print(\"--\")\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  #train the model\n",
        "  history = modelCNN.fit(x2[train],y[train],epochs=10)\n",
        "  scores = modelCNN.evaluate(x2[test],y[test],verbose=2, batch_size=10)\n",
        "  print(f'Score for fold {fold_no}: {modelCNN.metrics_names[0]} of {scores[0]}; {modelCNN.metrics_names[1]} of {scores[1]}')\n",
        "  acc_per_fold.append(scores[1])\n",
        "  loss_per_fold.append(scores[0])\n",
        "  \n",
        "  start = time.process_time();\n",
        "  y_pred1 = modelCNN.predict(x2[test])\n",
        "  end = time.process_time();\n",
        "  print(\"predictTime: \", end - start)\n",
        "  y_pred = np.argmax(y_pred1, axis=1)\n",
        "  # Print f1, precision, and recall scores\n",
        "  precision_score_ = precision_score(y[test], y_pred , average=\"macro\")\n",
        "  recall_score_ = recall_score(y[test], y_pred , average=\"macro\")\n",
        "  f1_score_ = f1_score(y[test], y_pred , average=\"macro\")\n",
        "  confmat_ = confusion_matrix(y[test],y_pred, labels=[0, 1, 2, 3, 4, 5, 6])\n",
        "  #confusion_matrix(y_true, y_pred)\n",
        "  print(precision_score_)\n",
        "  print(recall_score_)\n",
        "  print(f1_score_)\n",
        "  print(confmat_)\n",
        "  prec_per_fold.append(precision_score_)\n",
        "  recall_per_fold.append(recall_score_)\n",
        "  f1_per_fold.append(f1_score_)\n",
        "  confmat_per_fold = np.add(confmat_per_fold, confmat_) \n",
        "  predictTimeTotal += end - start;\n",
        " \n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "print(\"acc: \", np.mean(acc_per_fold))\n",
        "print(\"prec: \", np.mean(prec_per_fold))\n",
        "print(\"recall: \", np.mean(recall_per_fold))\n",
        "print(\"f1: \", np.mean(f1_per_fold))\n",
        "print(\"confusion matrix:\")\n",
        "print(confmat_per_fold)\n",
        "print(\"predictTimeTotal: \", predictTimeTotal)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check am i running at GPU\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "id": "ExG6weZJMob7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}